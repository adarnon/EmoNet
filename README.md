# EmoTe

## Audio-only Emotion Detection using Student-Teacher Distillation

## Proposal

- <https://docs.google.com/presentation/d/1_KhNUIATO6hUClqAUfZ_jjpc9R3Ij_ds4bHnsRWE4tg/edit?usp=sharing>

## Work Documentatino

- RAVDESS speech-to-text done with Deepspeech: <https://github.com/mozilla/DeepSpeech>

## Useful

- Speech-to-text: <https://colab.research.google.com/github/scgupta/yearn2learn/blob/master/speech/asr/python_speech_recognition_notebook.ipynb>

## Datasets

- [RAVDESS](https://github.com/robertjkeck2/EmoTe/tree/master/data/RAVDESS)
- [SAVEE](https://github.com/robertjkeck2/EmoTe/tree/master/data/SAVEE)
- [IEMOCAP](https://github.com/robertjkeck2/EmoTe/tree/master/data/IEMOCAP)

## References

- <https://arxiv.org/pdf/1503.02531.pdf>
- <https://arxiv.org/ftp/arxiv/papers/1802/1802.06209.pdf>
- <https://personal.utdallas.edu/~john.hansen/Publications/CP-ICASSP13-KaushikSangwanHansen-Sentiment-0008485.pdf>
- <https://github.com/shaharpit809/Audio-Sentiment-Analysis>
- <https://arxiv.org/pdf/1904.08138v1.pdf>
- <https://zenodo.org/record/1188976>
- <https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer>
- <https://github.com/tyiannak/pyAudioAnalysis>
- <https://github.com/pyannote/pyannote-audio>
- <http://kahlan.eps.surrey.ac.uk/savee/Database.html>
- <https://github.com/laugustyniak/awesome-sentiment-analysis>
- <http://www.robots.ox.ac.uk/~vgg/research/cross-modal-emotions/>
- <http://www.robots.ox.ac.uk/~vgg/demo/theconversation/>
- <https://sentic.net/benchmarking-multimodal-sentiment-analysis.pdf>
- <https://github.com/PiotrSobczak/speech-emotion-recognition>
- <https://sail.usc.edu/iemocap/>
- <http://immortal.multicomp.cs.cmu.edu/raw_datasets/processed_data/>

## Citations

- Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.
- S. Haq and P.J.B. Jackson, "Multimodal Emotion Recognition", In W. Wang (ed), Machine Audition: Principles, Algorithms and Systems, IGI Global Press, ISBN 978-1615209194, chapter 17, pp. 398-423, 2010.
- S. Haq and P.J.B. Jackson. "Speaker-Dependent Audio-Visual Emotion Recognition", In Proc. Int'l Conf. on Auditory-Visual Speech Processing, pages 53-58, 2009.
- S. Haq, P.J.B. Jackson, and J.D. Edge. Audio-Visual Feature Selection and Reduction for Emotion Classification. In Proc. Int'l Conf. on Auditory-Visual Speech Processing, pages 185-190, 2008
- C. Busso, M. Bulut, C.C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J.N. Chang, S. Lee, and S.S. Narayanan, "IEMOCAP: Interactive emotional dyadic motion capture database," Journal of Language Resources and Evaluation, vol. 42, no. 4, pp. 335-359, December 2008.
